# 47. Control Plane Failure 모의고사



(다시 풀어보기)



1. The cluster is broken again. We tried deploying an application but it's not working. Troubleshoot and fix the issue.

   Start looking at the deployments.

   - Fix the cluster

   **Hint**

   - Check the status of all control plane components and identify the component's pod which has an issue.

   **Solution**

   - Run the command: `kubectl get pods -n kube-system`. Check the `kube-scheduler` manifest file and fix the issue.
     The command run by the scheduler pod is incorrect. Here is a snippet of the YAML file.

     ```yaml
     spec:
       containers:
       - command:
         - kube-scheduler
         - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
         - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
         - --bind-address=127.0.0.1
         - --kubeconfig=/etc/kubernetes/scheduler.conf
         - --leader-elect=true
         - --port=0
     ```

     Once this is corrected, the scheduler pod will be recreated.

```
$ kubectl get all
$ kubectl get pod -n kube-system
$ kubectl describe pod [pod이름] -n kube-system
$ cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
# 아래에서 왜 staticPodPath를 찾지? -> kube-scheduler.yaml 위치 찾으려고!?
$ grep -i staticPodPath /var/lib/kubelet/config.yaml 
staticPodPath: /etc/kubernetes/manifests
$ cd /etc/kubernetes/manifests
$ ls
$ vi kube-scheduler.yaml
# 수정하면 정적 파드이지만 파드를 다시 생성합니다.

$ watch kubectl get all
```



2. Scale the deployment `app` to 2 pods.

   - Scale Deployment to 2 PODs

   **Hint**

   - Run the command: `kubectl scale deploy app --replicas=2`

   

   

3. Even though the deployment was scaled to 2, the number of `PODs` does not seem to increase. Investigate and fix the issue.

   Inspect the component responsible for managing `deployments` and `replicasets`.

   - Fix issue
   - Wait for deployment to actually scale

   **Hint**

   - Check the status of all control plane components and identify the component's pod which has an issue.

   **Solution**

   - Run the command: `kubectl get po -n kube-system` and check the logs of `kube-controller-manager pod` to know the failure reason by running command: `kubectl logs -n kube-system kube-controller-manager-controlplane`
   Then check the `kube-controller-manager` configuration file at `/etc/kubernetes/manifests/kube-controller-manager.yaml` and fix the issue.

   ```sh
   root@controlplane:/etc/kubernetes/manifests# kubectl -n kube-system logs kube-controller-manager-controlplane
   Flag --port has been deprecated, see --secure-port instead.
   I0725 07:25:16.842138       1 serving.go:331] Generated self-signed cert in-memory
   stat /etc/kubernetes/controller-manager-XXXX.conf: no such file or directory
   root@controlplane:/etc/kubernetes/manifests# 
   ```

   The configuration file specified (`/etc/kubernetes/controller-manager-XXXX.conf`) does not exist.
   Correct the path: `/etc/kubernetes/controller-manager.conf`

   ```
   $ kubectl get pods -n kube-system
   $ cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
   $ grep -i staticPodPath /var/lib/kubelet/config.yaml
   $ cd /etc/kubernetes/manifests
   $ ls
   $ pwd
   $ kubectl -n kube-system describe kube-controller-manager.yaml
   error: the server doesn't have a resource type "kube-controller-manager"
   $ kubectl -n kube-system logs kube-controller-manager-master
   ~~~~~~~~~~~~~~
   ~~~~~~~~~~~~~~
   $ cd /etc/kubernetes
   $ ls
   $ cd -
   /etc/kubernetes/manifests
   # 확인후 수정
   $ vi kube-controller-manager.yaml
   $ kubectl get pods -n kube-system
   $ kubectl get pods
   # 확인후 check!
   ```



4. Something is wrong with scaling again. We just tried scaling the deployment to 3 replicas. But it's not happening.

   Investigate and fix the issue.

   - Fix Issue
   - Wait for deployment to actually scale

   **Hint**

   - Use the command `kubectl get pods -n kube-system` and check the logs of one of the fail control plane component and try to fix the cause of issue.

   **Solution**

   - Check the volume mount path in `kube-controller-manager` manifest file at `/etc/kubernetes/manifests`.
     Just as we did in the previous question, inspect the logs of the `kube-controller-manager`pod:

     ```sh
     root@controlplane:/etc/kubernetes/manifests# kubectl -n kube-system logs kube-controller-manager-controlplane
     Flag --port has been deprecated, see --secure-port instead.
     I0725 07:29:06.155330       1 serving.go:331] Generated self-signed cert in-memory
     unable to load client CA file "/etc/kubernetes/pki/ca.crt": open /etc/kubernetes/pki/ca.crt: no such file or directory
     root@controlplane:/etc/kubernetes/manifests# 
     ```

     It appears the path `/etc/kubernetes/pki` is not mounted from the controlplane to the `kube-controller-manager` pod. If we inspect the pod manifest file, we can see that the incorrect hostPath is used for the volume:
     `WRONG`

     ```yaml
     - hostPath:
           path: /etc/kubernetes/WRONG-PKI-DIRECTORY
           type: DirectoryOrCreate
     ```

     
     `CORRECT`: ``yaml

     - hostPath: path: /etc/kubernetes/pki type: DirectoryOrCreate ``` Once the path is corrected, the pod will be recreated and our deployment should eventually scale up to 3 replicas.

   ```
   $ kubectl get pods -n kube-system
   ~~
   $ kubectl -n kube-system describe pods kube-controller-manager-master
   $ kubectl -n kube-system logs kube-controller-manager-master
   ~~
   ~~
   $ #vi kube-controller-manager.yaml
   $ pwd
   /etc/kubernetes/manifests
   $ vi kube-controller-manager.yaml
   $ kubectl -n kube-system logs kube-controller-manager-master
   $ $ vi kube-controller-manager.yaml
   ..
   ..
   ..
   ```

   