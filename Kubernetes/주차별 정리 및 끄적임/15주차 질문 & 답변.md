# 15주차 질문 & 답변



인강 (218 ~ 243)



1. **Design a Kubernetes Cluster**

   - GCP의 클러스터, Azure의 AWS 또는 AKS. 에 대해 이야기하자

     생산 수준 클러스터. 프로덕션 등급 애플리케이션 호스팅용.

     여러 마스터 노드가 있는 고가용성 다중 노드 클러스터가 권장됩니다.

     

     Google Container 엔진을 사용하면 GCP에서 Kubernetes 클러스터를 매우 쉽게 프로비저닝할 수 있습니다.

2. **Choosing Kubernetes Infrastructure**

   - 그렇다면 로컬 머신에서 kubernetes를 쉽게 시작하는 데 사용할 수 있는 솔루션에는 어떤 것이 있습니까?

     Minikube는 단일 노드 클러스터를 쉽게 배포합니다. Oracle과 같은 가상화 소프트웨어 중 하나에 의존합니다.

   - TurnKey Solutions

     - OpenShift
     - Cloud Foundry Container Runtime
     - VMware Cloud PKS
     - Vagrant

   - Hosted Solutions

     - Google Container Engine(GKE)
     - OpenShift Online
     - Azure Kubernetes Service
     - Amazon Elastic Container Service for Kubernetes(EKS)

   TurnKey와 Hosted  Solution에서 장단점을 찾아보진 않았지만 가장 선호하는 것은 설문조사를 기반하여 VirtualBox를 사용하는 것이라고 강의에서 알려줌. (Master Node를 나의 컴퓨터로하고 Worker Node를 VirtualBox를 사용)

3. **Configure High Availability**

   **API Server**가 요청을 수신하고 처리하거나 정보를 제공하는 책임이 있다.

   - 모든 클러스터 노드의 API 서버는 동시에 활성 상태로 실행될 수 있다.
   - kubectl 유틸리티가 작업을 완료하기 위해 API 서버와 통신한다.
   - API 서버가 수신 대기하는 곳이며 kube-config 파일에서 구성된다.

   **Controller Manager**

   - 클러스터의 상태를 보고 조치를 취하는 컨트롤러이다.

   - 프로세스

     > 기본적으로 true로 설정된 리더 선택 옵션을 지정할 수 있습니다.
     >
     > 이 옵션을 사용하면 컨트롤러 관리자 프로세스가 시작될 때 임대 또는 잠금을 얻으려고 시도합니다.
     >
     > kube-controller-manager 끝점으로 명명된 kubernetes의 끝점 개체입니다. 어느 것이 먼저 처리되는지
     >
     > 끝점을 업데이트합니다.
     >
     > 이 정보로 임대를 얻고 둘 중 활성이 됩니다.
     >
     > 다른 하나는 소극적 상태가 되어 Leader-elect-lease-를 사용하여 지정된 임대 기간 동안 잠금을 유지합니다.
     >
     > 지속 시간 옵션은 기본적으로 15초로 설정됩니다.
     >
     > 그런 다음 활성 프로세스는 옵션의 기본값인 10초마다 임대를 갱신합니다.
     >
     > 지도자 당선자 갱신 기한. 두 프로세스 모두 2초마다 리더가 되려고 시도합니다.
     >
     > Leader-elect-retry-period 옵션.
     >
     > 그런 식으로 한 프로세스가 실패하면 첫 번째 충돌이 필요하기 때문에 두 번째 프로세스가 획득할 수 있습니다.
     >
     > 자물쇠를 채우고 리더가 됩니다. 스케줄러는 유사한 접근 방식을 따르고 동일한 명령줄을 사용합니다.

   **Scheuler**

   - Controller Manager는 지속적으로 상태를 감시하는 복제 컨트롤러와 같은 컨트롤러로 구성된다.
   - 새로운 POD 생성과 같은 필요한 조치를 취한다.
   - 하나가 실패할 때. 여러 인스턴스가 병렬로 실행되면 작업이 중복될 수 있다.(병렬로 실행 X), 스케줄러도 마찬가지이다.

   **ETCD**

   - 2가지 토폴로지 구성

     > 하나는 여기에서 보이는 것과 동일한 아키텍처이며 이 과정을 통해 따라왔습니다.
     >
     > ETCD는 kubernetes 마스터 노드의 일부입니다. 이를 스택형 제어 계획 노드 토폴로지라고 합니다.
     >
     > 이것은 설정하기 쉽고 관리하기 쉽습니다. 그리고 더 적은 수의 노드가 필요합니다. 그러나 하나의 노드가 다운되면
     >
     > ETCD 구성원과 제어 평면 인스턴스가 모두 손실되고 중복성이 손상됩니다. 

     > 여기서 ETCD는 제어 평면 노드와 분리되어 자체 서버 집합에서 실행됩니다. 이것은 토폴로지입니다.
     >
     > 외부 ETCD 서버와. 이전 토폴로지와 비교
     >
     > 고장난 컨트롤 플레인 노드가 ETCD 클러스터 및 저장하는 데이터에 영향을 미치지 않기 때문에 이는 덜 위험합니다.
     >
     > 그러나 설정이 더 어렵고 외부 etcd 노드에 대해 두 배의 서버가 필요합니다. 그래서
     >
     > API 서버는 ETCD 서버와 통신하는 유일한 구성 요소입니다.
     >
     > API 서비스 구성 옵션, ETCD 서버의 위치를 지정하는 옵션 세트가 있습니다.
     >
     > 따라서 우리가 사용하는 토폴로지와 ETCD 서버를 구성하는 위치에 관계없이 동일한 서버의 날씨
     >
     > 또는 별도의 서버에서.
     >
     > 궁극적으로 우리는 API 서버가 ETCD 서버의 올바른 주소를 가리키고 있는지 확인해야 합니다.
     >
     > 이제 ETCD는 분산 시스템이므로 API 서버 또는 통신하려는 기타 구성 요소를 기억하십시오.
     >
     > 모든 인스턴스에서 ETCD 서버에 연결할 수 있습니다.
     >
     > 사용 가능한 ETCD 서버 인스턴스를 통해 데이터를 읽고 쓸 수 있습니다.
     >
     > 이것이 kube-apiserver 구성에서 etcd-servers 목록을 지정하는 이유입니다.

4. **ETCD In HA**

   - 키, 값 으로 구성되어 있다.

   - 모든 데이터를 읽을 수 있지만 쓰기의 경우는 아니다. ETCD는 각 노드의 쓰기를 동시에 처리하지 않는다. 대신 인스턴스 중 하나만 쓰기 처리를 한다. 만약 두 노드가 있다면 그 중에서 리더를 선택한다. 나머지 노드는 팔로워가 된다.

   -  ETCD는 RAFT 프로토콜을 사용하여 분산 합의를 구현한다.

     > 요청을 시작하기 위한 임의의 타이머.
     >
     > 예를 들어 임의의 타이머는 타이머를 완료한 첫 번째 관리자가 보낸 세 명의 관리자에서 시작됩니다.
     >
     > 리더가 되기 위한 권한을 요청하는 다른 노드에 대한 요청을 출력합니다. 수신에 다른 관리자
     >
     > 요청은 투표로 응답하고 노드는 리더 역할을 맡습니다. 이제 선출되었으니
     >
     > 리더는 정기적으로 다른 마스터에게 계속 진행 중임을 알리는 알림을 보냅니다.
     >
     > 리더의 역할을 맡게 됩니다. 다른 노드가 리더로부터 알림을 받지 못하는 경우
     >
     > 리더가 다운되거나 네트워크 연결이 끊어져서 발생할 수 있는 특정 시점에
     >
     > 노드는 그들 사이에서 재선 과정을 시작하고 새로운 리더가 다시 식별됩니다.
     >
     > 권리가 표시되는 이전 예에서 리더에 의해 처리되고 다른 사람에게 복제됩니다.
     >
     > 클러스터의 노드는 권한이 다른 노드에 복제된 후에만 완료된 것으로 간주됩니다.

   - 우리는 ETCD 클러스터가 고가용성이다. 따라서 노드를 잃어도 여전히 작동해야 한다.

   - Quorum = N/2 + 1 —-> 항상 홀수

   - 내결함성을 위해 필요한 최소 노드 수는 3이다.

