# 13주차 질문 & 답변

인강 165~180 (Storage) 190~206 (Networking)



#### Storage

종류: AUFS, ZFS, BTRFS, Device Mapper, Overlay, Overlay2

- **Docker Storage**

  - Storage Drivers
  - Volume Drivers

- **Docker Layered Architecture**

  1. *Base Ubuntu Layer(120MB)*
  2. *Changes in apt packages(306MB)*
  3. *Chages in pip packages(6.3MB)*
  4. Source code(229MB)
  5. Update Entrypoint(0MB)

  Docker File이 두개 있을 경우 Docker 빌드 명령을 실행하여 이 애플리케이션에 대한 새 이미지를 빌드할 때 처음 세 개 이후 두 응용 프로그램의 계층이 동일하면 Docker는 처음 세 계층을 빌드하지 않습니다.

  이러한 방식으로 Docker는 이미지를 더 빠르고 효율적으로 빌드하고 디스크 공간을 절약합니다.

  이는 애플리케이션을 업데이트할 때마다 애플리케이션 코드를 업데이트하는 경우에도 적용됩니다. 이 경우 Docker는 캐시에서 모든 이전 레이어를 재사용하고 애플리케이션을 빠르게 재구축합니다. 최신 소스 코드를 업데이트하여 이미지를 업데이트하여 다시 빌드 및 업데이트하는 동안 많은 시간을 절약할 수 있습니다.

  ![image](https://user-images.githubusercontent.com/55625864/137577991-958e3d16-21fc-4e98-a94f-c4dd38a7e4cd.png)

  **참조**

  - https://kin3303.tistory.com/22

- **Docker Volume**

  컨테이너 레이어에 데이터는 컨테이너의 실행이 끝나 컨테이너가 삭제되면 사라집니다.

  그렇다면 데이터를 보존하려고 한다면 어떻게 해야 할까요?

  컨테이너에 영구 볼륨을 추가해야 합니다. 그리고 이것이 도커 볼륨입니다.

  - Docker Volume 
  - Docker Data Volume
  - Host Docker Data Volume
  - Named Docker Data Volume
  - Docker Data Volume Container

  **참조**

  - https://kin3303.tistory.com/23?category=886468

  볼륨은 스토리지 드라이버에 의해 처리되지 않는다는 점을 기억하십시오.

  볼륨은 볼륨 드라이버 플러그인에 의해 처리됩니다.

**참조**

- https://www.joinc.co.kr/w/man/12/docker/storage



##### 오케스트레이션 솔루션

오케스트레이션 솔루션은 컨테이너를 쉽고 빠르게 배포, 확장, 관리를 자동화해주는 플랫폼입니다.

#### CSI (Container Storage Interface)

도커 말고도 Container interface를 만들어서 다른것을 사용할 수 있게 되었다.  Network, host 스토리지를 사용할 수 있다.

> 스토리지 시스템에 대한 지원을 Kubernetes에 추가(또는 기존 볼륨 플러그인의 버그 수정)하려는 공급업체는 Kubernetes 릴리스 프로세스에 맞춰야 했습니다. 또한 타사 저장소 코드로 인해 핵심 Kubernetes 바이너리에서 안정성 및 보안 문제가 발생했으며 코드는 Kubernetes 유지 관리자가 테스트 및 유지 관리하기 어려운 경우가 많았습니다(경우에 따라 불가능).
>
> CSI는 Kubernetes와 같은 컨테이너 오케스트레이션 시스템(CO)의 컨테이너화된 워크로드에 임의의 블록 및 파일 스토리지 시스템을 노출하기 위한 표준으로 개발되었습니다. Container Storage Interface의 채택으로 Kubernetes 볼륨 계층은 진정으로 확장 가능하게 되었습니다. 타사 스토리지 공급자는 CSI를 사용하여 핵심 Kubernetes 코드를 건드릴 필요 없이 Kubernetes의 새 스토리지 시스템을 노출하는 플러그인을 작성하고 배포할 수 있습니다. 이는 Kubernetes 사용자에게 더 많은 스토리지 옵션을 제공하고 시스템을 보다 안전하고 안정적으로 만듭니다.
>
> 참조: https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/



#### PV & PVC

#### Storage Classes

![image](https://user-images.githubusercontent.com/55625864/137610947-572c1496-d9a6-4772-9437-6eb6de3b02aa.png)

![image-20211017131823828](C:\Users\seouz\AppData\Roaming\Typora\typora-user-images\image-20211017131823828.png)



#### Networking

**Pod Networking**

**서로 다른 노드의 Pod와 통신하기!**

![image](https://user-images.githubusercontent.com/55625864/137619219-30d8d0ec-ac94-47b5-8339-66e5227fb8fc.png)

![image](https://user-images.githubusercontent.com/55625864/137619395-4e4d8893-97cf-491e-b8b7-1b50b5d2616a.png)

- `ip link add v-net-0 type bridge`
  - 각 노드에 네트워크를 만듭니다.
- `ip link set dev v-net-0 up`
  - 브리지 인터페이스 또는 네트워크에 IP 주소를 할당
- `ip addr add 10.244.1.1/24 dev v-net-0`
  - 브리지 인터페이스의 IP 주소

여기까지 기본베이스를 구축합니다. 나머지 단계는 각 컨테이너에 대해 그리고 새 컨테이너가 생성될 때마다 수행되어야 합니다.



1. 우리는 IP 링크를 사용하여 생성하고 명령을 추가하고 옵션에 초점을 맞추지 마십시오.

   **net-script.sh**

   ```
   # Create veth pair
   ip link add .....
   
   # Attach veth pair
   ip link set .....
   ip link set .....
   
   # Assign IP Address
   ip -n <namespace> addr add .....
   ip -n <namespace> route add .....
   
   # Bring Up Interface
   ip -n <namespace> link set .....
   ```

2. 그런 다음 해당 명령의 IP 링크를 사용하여 한쪽 끝을 컨테이너에 연결하고 다른 쪽 끝을 브리지에 연결합니다.

3. 그런 다음 IP ADDR 명령을 사용하여 IP 주소를 할당하고 기본 게이트웨이에 경로를 추가합니다.

4. 마지막으로 인터페이스를 불러옵니다.

이번에는 정보가 있는 두 번째 컨테이너에서 동일한 스크립트를 실행하지 않고 컨테이너를 가져옵니다.

네트워크에 연결된 두 컨테이너는 이제 서로 통신할 수 있습니다.

스크립트를 다른 노드에 복사하고 해당 노드에서 스크립트를 실행하여 IP 주소를 할당하고 연결합니다.

컨테이너를 자체 내부 네트워크에 연결합니다. 포드는 모두 고유한 IP 주소를 가지며 자체적으로 서로 통신할 수 있습니다.

<img src="C:\Users\seouz\AppData\Roaming\Typora\typora-user-images\image-20211017204845073.png" alt="image-20211017204845073" style="zoom:70%;" />



node1의 10.244.1.2의 pod와 nod2의 10.244.2.2 pod에 연결하기 위해서는 `ip route add 10.244.2.2 via 192.168.1.12`로 연결을 할 수 있게 합니다.

경로가 추가 되면 10.244.1.2 pod에서 바로 10.244.2.2로 연결을 할 수 있습니다. 아래와 같이 다른 파드들도 모두 route 해줍니다.

<img src="https://user-images.githubusercontent.com/55625864/137626157-ab293798-03c9-46a7-99a2-31283ce01245.png" alt="image" style="zoom:70%;" />

더 나은 솔루션은 네트워크에 라우터가 있고 모든 호스트가 사용하도록 지정하는 경우 라우터에서 수행하는 것입니다. 기본 게이트웨이로 사용합니다. 그렇게 하면 라우터의 라우팅 테이블에 있는 모든 네트워크에 기록된 내용을 쉽게 관리할 수 있습니다. 그것으로 주소 10.244.0.0으로 생성한 개별 가상 네트워크를 만들었습니다.

<img src="https://user-images.githubusercontent.com/55625864/137626998-8109c868-4117-4288-bb5a-4b9173e89865.png" alt="image" style="zoom:80%;" />



네트워크에 컨테이너 추가를 처리하는 광고 섹션과 삭제 섹션이 있어야 합니다.

네트워크에서 컨테이너 인터페이스를 삭제하고 IP 주소를 해제하는 등의 작업을 처리합니다.

**net-script.sh**

```
ADD)
# Create veth pair
# Attach veth pair
# Assign IP Address
# Bring Up Interface
ip -n <namespace> link set .....
DEL)
# Delete veth pair
ip link del .....
```

각 노드의 kubelet은 컨테이너가 생성될 때마다 컨테이너 생성을 담당합니다. 큐브가 실행될 때 명령줄 인수로 전달된 CNI 구성을 보고 식별합니다.

<img src="https://user-images.githubusercontent.com/55625864/137626991-868441b1-13f2-4f3e-808e-bf498e827c66.png" alt="image" style="zoom:80%;" />



#### Container Networking Interface(CNI) in k8s

CNI는 컨테이너 런타임의 책임을 정의합니다. 컨테이너 런타임, 네트워크 네임스페이스 생성, 식별 및 올바른 네트워크 플러그인을 호출하여 해당 네임스페이스를 올바른 네트워크에 연결합니다.

CNI 플러그인은 클러스터의 각 노드에서 kubelet  서비스에 구성됩니다.

<img src="https://user-images.githubusercontent.com/55625864/137627244-bb4b2070-4d1d-444b-aed8-41684149d5fb.png" alt="image" style="zoom:90%;" />

kubelet 서비스 파일을 보면 network-plugin이라는 옵션이 CNI로 설정된 것을 볼 수 있습니다.

![image](https://user-images.githubusercontent.com/55625864/137627403-d1447977-b293-4791-9973-3dcb6d774109.png)

실행 중인 kubelet 서비스를 볼 때도 동일한 정보를 볼 수 있습니다.

CNI로 설정된 네트워크 플러그인과 CNI bin과 같은 CNI와 관련된 몇 가지 기타 옵션을 볼 수 있습니다.

디렉토리 및 CNI Config 디렉토리 CNI bin 디렉토리에는 지원되는 모든 CNI 플러그인이 실행 파일로 있습니다. bridge, dhcp, flannel과 같은 등 CNI 충돌 디렉토리에는 구성 파일 세트가 있습니다. 이것은 kubelet이 어떤 플러그인을 사용해야 하는지 찾는 곳입니다. 이 경우 브리지 구성 파일을 찾습니다. 여기에 여러 파일이 있는 경우 알파벳 순서로 하나를 선택합니다.



<img src="C:\Users\seouz\AppData\Roaming\Typora\typora-user-images\image-20211017214026270.png" alt="image-20211017214026270" style="zoom:80%;" />

Bridge conf 파일을 보면 이런 모습입니다.

플러그인 구성 파일에 대해 CNI 표준에서 정의한 형식입니다.



#### WeaveWorks(CNI)

<img src="https://user-images.githubusercontent.com/55625864/137626991-868441b1-13f2-4f3e-808e-bf498e827c66.png" alt="image" style="zoom:80%;" />

자체 사용자 정의 CNI 스크립트가 있습니다. CNI를 통해 kubelet에 구축하고 통합했습니다. 사용자 정의 스크립트에서 weave 플러그인을 통합할 수 있습니다.

weave CNI 플러그인이 클러스터에 배포되면 각 노드에 에이전트 또는 서비스를 배포합니다. 그들은 정보를 교환하기 위해 서로 통신합니다. 노드와 네트워크 및 그 안의 POD에 관한 것입니다. 각 에이전트 또는 피어는 전체 설정의 토폴로지를 저장하고, 그렇게 하면 다른 노드에 있는 포드와 해당 IP를 알 수 있습니다.

Weave는 노드에 자체 브리지를 만들고 이름을 지정합니다. weave 그런 다음 각 네트워크에 IP 주소를 할당합니다.





---

---

---

---

13주차 (2021-10-17 20:00)
(Docker Reminder)

1. layers - 종류와 특징

- container layer 

  -  read/write 
  - new commits on the image layer (modifications) 
  - life cycle goes hand in h 

- image layer 

  - base layer 
  - read only 
  - shared

  and with containers

-  \* layered architecture의 장점 

- cache  

  = 같은 layer를 유지하는 부분까지는 캐시를 사용해서 더 빠르게 빌드할 수 있다. (save time & resources)

2. Volumes 

- 볼륨이 무엇인지 
- 데이터를 영구적으로 저장하기 위한 장소? (storage to persist data)
- 볼륨이 왜 필요한지  
  - 컨테이너에 저장된 데이터는 컨테이너가 삭제되면 함께 사라지는 것이 기본적인 구조  
  - 하지만 볼륨을 사용해 호스트 경로에 연결해 놓으면, 지정된 경로에 데이터가 연결되어 저장되기 때문에, 컨테이너가 사라져도 데이터가 보존된다.

3.  Storage Driver 

- Storage Driver 덕분에 layered architecture를 구현할 수 있다고 한다. (from 인강) - 여러 종류의 스토리지 드라이버가 있으며, 성능 & 지원 내용 등이 다르므로 필요에 맞는 스토리지 드라이버를 고를필요가 있다.

4. volume Driver 

- - 볼륨을 관리하는 플러그인 정도?

5. CSI (Container Storage Interface) 

- CSI의 필요성이 대두된 이유? 

  CRI가 생겨난 이유와 비슷하다.  

  - 과거, k8s는 도커에 embedded 된 형태  

  - 여러 컨테이너 솔루션들이 등장하기 시작  

  - k8s가 한 컨테이너 솔루션(도커)에 종속되는 것을 피하고자, 여러 솔루션으로의 자유로운 변경을 위해서 CRI(Container Runtime Interface)가 등장했다.  

  - 이제 CRI를 구현하기만 하면, 어떤 컨테이너든지 k8s를 사용할 수 있고, k8s 또한 CRI 덕분에 솔루션에서 솔루션으로 전환 시에도 소스 코드 등에 변화가 필요가 없다.

  - CSI 또한 같은 궤도를 탄다.

    = 여러 스토리지 솔루션들 간의 자유로운 스위치를 위해서  

    = CSI를 구현한 스토리지면 k8s를 사용할 수 있다.  

    (CSI는 k8s only가 아니다. = universal standard)

6. Volume (in k8s) 

- 쿠버네티스에서의 볼륨? 
- 도커 때와 같은 범주안에 있다. (데이터의 영구? 보관)

- how to configure volumes 

  1) .yaml 파일에 정의 

- spec.containers 밑 & spec.volumes 밑에 정의해서 쓸 수 있다. 

- 단점: 모든 파일에 정의를 해야 한다.   

  - 하나라도 변화가 있다면...?

  - 2) PV & PVC 

    - PV 

      - cluster-wide, 스토리지 볼륨들의 pool이라고 할 수 있다. 

      - 관리자에 의해 만들어지며, 사용자(개발자)에 의해 사용된다.  

        = PVC에 정의해서 바인딩 해 사용한다.

    - PVC

      - PVC정의는 깜빡했는데 다들 복습하는 내용이라… 따로 추가 안하고 이대로 두겠습니다! 

    - 위 조합의 장점? 

      - 하나를 바꾼다고 해서 여기저기 다 바꿀 필요가 없다.  

        = PVC가 PV를 사용하는 형태기 때문에, PV 내용을 수정하면 해당 수정사항이 모두 반영된다.

    - PVC에 PV 사용 시 주의할 점 

      - 필요한 PV의 사양이 적다고 해도, 남는 PV가 없으면 고사양의 PV가 자동으로 매핑될 수 있다. 

      - PVC에 정의한 옵션에 맞는 PV가 여러 개 존재한다면?  

        = labels, selectors를 사용해 제약을 걸 수 있다. 

    - 사용가능한 PV가 없다면?  

      = 사용가능한 PV가 생길 때까지 Pending 상태에 걸려있게 된다.

    - accessModes 종류 (Reminder) 

      1) ReadOnlyMany 

      2) ReadWriteOnce 

      3) ReadWriteMany

    - Reclaim Policy 

      = PV에 바인딩 된 PVC가 삭제되었을 때, PV를 어떻게 처리할 것인가?

      1) Retain 

      - 관리자가 수동으로 삭제할 때까지 남아있다. 
      - 다른 PVC에 의해서 재사용불가능

      2) Delete 

      - 자동으로 PV를 지운다. 

      - 당연히 데이터는 모두 사라진다.

      3) Recycle 

      - data volume에 있는 데이터가 모두 초기화 된다. (다른 PVC에 의해 사용가능해지기 전에)

      - Static Provisioning vs Dynamic Provisioning 

      1) Static ~ 

      - 수동으로 사용가능한 스토리지, pv, pvc 각각 생성

       2) Dynamic ~

      - 정해진 provisioner를 사용해서 자동으로 필요한 스토리지를 생성한다. 

      - Storage Class와 함께 사용한다.  

        = 스토리지 클래스를 정의해서, PVC 생성 시 yaml 파일에 스토리지 클래스를 명시해두면, provisioner가 자동으로 필요한 스토리지 & PV를 생성해서 바인딩해준다.

7. 클러스터 네트워킹을 위해 필요한 것 

   1) 인터페이스 

   - 각 노드는 네트워크에 연결된 적어도 하나의 인터페이스를 가지고 있어야 한다. 

   2) ip 주소 

   - 각 인터페이스는 ip 주소가 설정되어 있어야 한다. 

   3) unique hostname, MAC address 

   4) ports 

   - 정해진 포트가 열어져 있어야 한다. (ex. 6443 on master, 10250 on kublet ....)

8. 포드 네트워킹

   = 여러 클러스터에 걸친 많은 수의 포드가 서로 커뮤니케이션 할 수 있는 방법

   *1) Networking Model for Pods* 

   - 각 포드는 IP 주소를 가지고 있어야 한다. 
   - 각 포드는 다른 노드에 있는 다른 모든 포드에 접근할 수 있어야 한다. (해당 IP 주소를 사용해서)
     * 각 노드 간 연결을 위해 브릿지 네트워크를 설정하고, 아이피 주소를 부여할 수 있다.   각 컨테이너에도 ip 할당이 가능하다.


   *2) 이때 문제점?* - 시스템의 규모가 점점 커진다면 어떻게 모든 컴포넌트들에 ip주소를 각각 부여할 것인가? - 부여된 주소를 수정/제거할 필요가 있다면?  = maintanence issues

   *3) 해결책?*  

   라우터

   = 모든 요청을 한 곳에서 받도록 하는 듯 하다?  

   스크립트 사용   

   = CNI(Container Network Interface)를 사용한 스크립트를 만들어 실행?

- CNI with container runtime의 책임/역할   

  컨테이너 런타임은   

  (1) 네트워크 네임스페이스를 생성해야 한다.   

  (2) 네트워크를 식별해서 컨테이너에 붙여줘야 한다.   

  (3) 네트워크 플러그인을 실행해야 한다. (컨테이너가 추가되고 삭제될 때)

  

  4) 컨테이너가 생성되었을 때 kubelet이 하는일 (해당 컨테이너에 네트워크 설정을 위해)  

  - 커맨드라인 인자로 넘겨진 cni 설정을 파악한 뒤, cni bin 디렉터리에 있는 스크립트를 찾아서 실행해준다.

  5) 한 포드에서 다른 포드로 네트워크 패킷 전송하기! 

  (1) simple way   

  - 포드 -> 노드 -> 라우터 -> 다른 노드  

  = 심플한 환경에서는 잘 작동한다. 하지만 네트워크 환경이 점점 커진다면?  라우팅 테이블이 일단, 많은 엔트리를 받지 못할 수 있다.

  (2) 에이전트 사용해서 패킷 전송하기 

  (hint! shipping 비유)  

  - 에이전트를 각 노드에 둔다.

    * 이때, 에이전트의 역할  

      = 패킷을 전송하는 모든 활동을 담당

      = 각각의 에이전트가 잘 연결되어 있는지 확인

    - 포드 -> 패킷 -> 에이전트  
    - 에이전트에서 캡슐화(새로운 패킷으로 변환 with 새로운 목적지)
    - 에이전트 -> 목적지 노드의 에이전트  
    - 목적지 노드의 에이전트에서 캡슐화 해제  
    - 목적지 노드의 에이전트 -> 대상 포드

  Weave 설치하기 

  (알아두면 좋다고 하셔서 솔루션에 있던 거 끼워넣어놨습니다. 공식문서에도 있던걸로 기억하는데 맞는지 모르겠네요) 

  - kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"

  IPAM (IP Address Management) 

  1) IPAM이 무엇인가?  

  - 포드가 어떻게 ip를 할당 받는지 

  - 누가 중복되는 ip가 없도록 보장해주는지   

    = CNI plugin, the network solution provider  

  -  노드의 가상 브릿지 네트워크가 어떻게 ip 서브넷을 할당받는지  

  - 어디에 이러한 정보들이 저장되는지   

    = IP 목록을 담아 파일에 저장하고, 해당 파일은 각 호스트에 둔다.

    = CNI가 내장된 플러그인을 통해 이러한 태스크를 수행하게 할 수 있다. (수동으로 각각 스크립트를 실행할 필요가 없어진다.)

    (여러 솔루션이 존재하며, 각 솔루션마다 다른 방식으로 동작한다. 대표적인게 인강에 등장했던 weave)

9. 서비스 네트워킹

   * Reminder! 서비스 타입에는 무엇이 있었는지? 3가지 

     (인강에는 두 가지만 언급되었으니, 두 가지만 말해도 상관없 을 것 같긴 하네요) 

   1) Cluster IP  

   - 같은 클러스터 내에서만 해당 서비스에 접근이 가능하다.

   2) NodePort  

   - ip 주소가 서비스에 주어진다. (외부에서 접근가능해진다.)  

   - 나머지 동작방식은 클러스터 IP와 같되, 포트를 열어 외부에서 접근이 가능하다.   

     = 물론 서비스로 요청을 직접 받기보단 인그레스를 통해 받은 뒤 할당했던 걸로 기억?   (인그레스 부분 기억 안나면 동작 방식? 정도는 remind 하고 가도 나쁘지 않을 것 같습니다!)

   3) LB  - 클라우드 서비스 등이 제공해주는 기본 LB에 서비스를 붙이는 방식이었던 것으로 기억  - 한정적이다. (해당 서비스를 지원해주는 플랫폼에서만 사용이 가능하다.)

   \* 서비스에 ip 규칙을 설정하는 방식? 

   (인강에서 듣기만 하고, 따로는 못찾아봐서 틀릴 수 있습니다 ㅜ) 

   1) userspace 

   - kube proxy가 각각의 서비스의 특정 포트를 listen 

   2) ipvs  

   - ipvs 규칙을 만들어 포드로의 연결을 대신(?) 보내줌 

   3) iptables (default)  

   - ip 테이블을 사용하는 방식

   

   \* 서비스가 ip를 할당받고 클러스터 전체(혹은 외부로 부터)로 접근 가능해지는 방식?  (vs 포드와 비교)   

   1) 포드   
   
   - 모든 노드에는 kubelet 프로세스가 실행중이다. (daemonSet 이었던 것으로 기억? Nope)    (kubelet이 포드를 생성하는 역할을 담당한다.)
     
   - kube-api 서버를 watch 하면서 변화를 감지한다.
  
     = 포드가 생성될 때마다, 노드에 포드를 생성하고 CNI 플러그엌ㅋㅋ인을 실행시켜 포드에 네트워크를 설정한다.

   2) 서비스   

   - 서비스는 가상의(virtual), 클러스터 범위(clsuter-wide)의 오브젝트이다.    

     = 포드처럼 노드에 생성되거나, 할당되지 않는다.  

   - 서비스가 생성될 때, 정해진 범위 내의 ip가 할당된다.  
   
     = `kube proxy`가 서비스가 생성/삭제될 때 규칙들을 붙이고 삭제한다.