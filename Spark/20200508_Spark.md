# 20200508 Spark

### Apache Hadoop MapReduce와 Apache Spark

SparkContext : Spark 애플리케이션과 클러스터의 연결을 관리하는 객체 RDD 등 Spark 에서 사용되는 주요 객체는 SparkContext 객체를 통해서 생성된다.

```java
SparkConf conf = new SparkConf().setMaster("local").setappName("xxx");
JavaSparkContext sc = new JavaSparkContext(conf);
```

RDD(Resilient Distributed DataSet)) :
Spark 에서 처리되는 데이터는 RDD 객체로 생성하여 처리한다.
RDD 객체는 두 가지 방법으로 생성 가능하다.
(1) Collection 객체를 만들어서
(2) HDFS 의 파일을 읽어서

- RDD 객체의 특징 : 
  - Read Only(immutable)
  - 1~n 개의 partition 으로 구성 가능
  - 병렬적(분산) 처리가 가능하다.
  - RDD의 연산은 Transformation 연산과 Action 연산으로 나뉜다.
  - Transformation은 Lazy-execution을 지원한다.
  - lineage를 통해서 fault tolerant(결함 내성)를 확보한다.

[ Transformation과 Action ]
- 연산의 수행 결과가 RDD 이면 Transformation
- Transformation 은 기존 RDD를 이용해 새로운 RDD를 생성하는 연산이다.
(변환, 필터링 등의 작업들 : 맵, 그룹화, 필터, 정렬...)
Lineage를 만들어 가는 과정이다.
- 연산의 수행 결과가 정수, 리스트, 맵 등 RDD 가 아닌 다른 타입이면 Action
(first(), count(), collect(), reduce()...)
Lineage를 실행하고 결과를 만든다.

Lazy-execution 이란 Action 연산이 실행되기 전에는 Tranformation 연산이 처리되지 않는 것을 의미한다. Transformation 연산은 관련 메서드를 호출하여 연산을 요청해도 실제 수행은 되지 않고 연산 정보만 보관한다. 이렇게 Transformation 연산 정보를 보관한 것을 Lineage(리니지)라고 한다. 보관만 하다가 첫 번째 Action 연산이 수행될 때 모든 Lineage 에 보관된 Transformation 연산을 한 번에 처리한다.
Lazy-execution과 Lineage를 활용함으로써 처리 효율을 높이고 클러스터 중 일부 고장으로 작업이 실패해도 Lineage를 통해 데이터를 복구한다.

RDD를 제어하는 API operation은 크게 2개의 타입
– Transformation : RDD에서 데이터를 조작하여 새로운 RDD를 생성하는 함수
– Action : RDD에서 RDD가 아닌 타입의 data로 변환하는 하는 함수들

operation의 순서를 기록해 DAG로 표현한 것을 Lineage라 부름



parallelize() : 스칼라컬렉션 객체를 이용해서 RDD 객체를 생성한다
count() : RDD의 요소 개수를 리턴한다
first() : RDD 객체의 첫 번째 요소를 리턴한다
collect() : RDD 객체의 모든 요소를 배열로 리턴한다
map() : 입력 RDD 의 모든 요소에 f 를 적용한 결과를 저장한 RDD 를 반환한다 .
flatMap() : 입력 RDD 의 모든 요소에 f 를 적용하고 모든 요소들을 하나로 묶어서 반환한다
filter() : 입력 RDD 의 모든 요소에 f 를 수행하고 참을 리턴하는 결과만 저장한 RDD 를 반환한다
reduce() : 지정된 f 를 수행시켜 입력 RDD 의 개수를 축소시켜서 생성된 RDD 를 반환한다
reduceByKey() : 키값을 기준으로 f 를 수행하고 RDD 를 반환한다
groupBy() : 입력 RDD 의 모든 요소에 f 를 적용한 결과로 그루핑된 RDD 를 반환 한다

#### SparkUtils.java

```java
package sparkexam;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;

public class SparkUtils {
	public static JavaSparkContext getSparkContext(String name) {
		SparkConf conf = new SparkConf().setAppName(name).setMaster("local");
		return new JavaSparkContext(conf);
	}
}
```

#### RDDOperate1.java

```java
package sparkexam;

import java.util.List;

import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

public class RDDOperate1 {

	public static void main(String[] args) throws Exception {

		JavaSparkContext sc = SparkUtils.getSparkContext("test1");
		JavaRDD<String> rdd = sc.textFile("hdfs://192.168.111.120:9000/edudata/fruits.txt");
		
		//textFile로 읽으면 행단위로 읽음
		System.out.println(rdd.first()); //action method, 첫번째 행
		
		System.out.println(rdd.take(3)); //action method, 원하는 개수 행 +만큼
		
		System.out.println(rdd.collect()); //action method, 전체를 List 객체로 리턴
		
		List<String> list = rdd.collect();
		for (String v : list) {
			System.out.println(v);
		}
		System.out.println(rdd.count());
		
		sc.stop();
	}
}
```

#### RDDOperate2.java

```java
package sparkexam;

import java.util.Arrays;

import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;

public class RDDOperate2 {

	public static void main(String[] args) throws Exception {
		JavaSparkContext sc = SparkUtils.getSparkContext("test2");
		doMap(sc);
		sc.stop();
	}

	public static void doMap(JavaSparkContext sc) {
		//병렬처리가 가능한 -> parallelize
		JavaRDD<Integer> rdd1 = sc.parallelize(Arrays.asList(1, 2, 3, 4, 5));

		// Java7
		// 새로운 rdd 객체를 만들어서 리턴
		// Function<Integer, Integer> : argument타입, return 타입
		JavaRDD<Integer> rdd2 = rdd1.map(new Function<Integer, Integer>() { 
			@Override
			public Integer call(Integer v1) throws Exception {
				return v1 + 10;
			}
		});

		// Java8 Lambda
		JavaRDD<Integer> rdd3 = rdd1.map(v1 -> v1 + 10);

		System.out.println("java 7 : " + rdd2.collect());
		System.out.println("java 8 : " + rdd3.collect());
	}	
	
}
```

### RDDOperate3.java

```java
package sparkexam;

import java.util.Arrays;
import java.util.List;

import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.VoidFunction;

public class RDDOperate3 {

	public static void main(String[] args) throws Exception {
		JavaSparkContext sc = SparkUtils.getSparkContext("test3");

		doForeach(sc);

		sc.stop();
	}

	public static void doForeach(JavaSparkContext sc) {
		List<Integer> data = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);
		JavaRDD<Integer> rdd = sc.parallelize(data);
		// Java7
		// VoidFunction : 리턴값이 없음
		rdd.foreach(new VoidFunction<Integer>() {
			@Override
			public void call(Integer t) throws Exception {
				System.out.println("data : " + t);
			}
		});
		// Java8
		rdd.foreach((Integer t) -> System.out.println("data : " + t));
	}	

	
}
```

#### RDDOperate4.java

```java
package sparkexam;

import java.util.ArrayList;
import java.util.List;

import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

public class RDDOperate4 {

	public static void main(String[] args) throws Exception {
		JavaSparkContext sc = SparkUtils.getSparkContext("test4");
		List<Integer> data = fillToN(100);
		JavaRDD<Integer> rdd = sc.parallelize(data);
		
		//Return this RDD sorted by the given key function.
		JavaRDD<Integer> rddSort = rdd.sortBy(f->f, false, 1); //2번째 아규먼트 sort 방식, 3번째 아규먼트 파티션 개수
		System.out.println(rddSort.take(10)); //앞에서부터 10개 가져옴
		
		Integer result1 = rdd.first();
		System.out.println(result1);
		
		List<Integer> result2 = rdd.take(5);
		System.out.println(result2);	
		
		sc.stop();
	}

	public static ArrayList<Integer> fillToN(int n) {
		ArrayList<Integer> rst = new ArrayList<>();
		for (int i = 0; i < n; i++)
			rst.add(i);
		return rst;
	}
	
}
```

#### RDDOperate5.java

```java
package sparkexam;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

public class RDDOperate5 {

	public static void main(String[] args) throws Exception {
		JavaSparkContext sc = SparkUtils.getSparkContext("test5");
		
		doMapAndFlatMap(sc);

		sc.stop();
	}

	public static void doMapAndFlatMap(JavaSparkContext sc) {
		List<String> data = new ArrayList<>();
		data.add("apple,orange");
		data.add("grape,apple,mango");
		data.add("blueberry,tomato,orange");
		JavaRDD<String> rdd1 = sc.parallelize(data); // 3개의 element로 구성되어 있음

		// split ruturn 값은 String 배열
		JavaRDD<String[]> rdd2 = rdd1.map((String t) -> t.split(","));
		// 모든 data를 하나의 Element로 만들어 버린다.
		//  flatMap 메서드는 차원을 줄인다.
		JavaRDD<String> rdd3 = rdd1.flatMap((String t) -> Arrays.asList(t.split(",")).iterator());
		
		System.out.println(rdd2.collect());
		System.out.println(rdd3.collect());
		
		JavaRDD<Integer> rdd4 = rdd1.map((String t) -> t.length());
		System.out.println(rdd4.collect());
		
	}	
}
```

#### RDDOperate6.java

```java
package sparkexam;

import java.util.Arrays;
import java.util.List;

import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

public class RDDOperate6 {

	public static void main(String[] args) throws Exception {
		JavaSparkContext sc = SparkUtils.getSparkContext("test6");

		doFilter(sc);

		sc.stop();
	}

	public static void doFilter(JavaSparkContext sc) {
		List<String> data = Arrays.asList("가나다", "abc", "ABC", "p111", "1234", "$@!%$");
		JavaRDD<String> rdd1 = sc.parallelize(data);
		
		JavaRDD<String> rdd2 = rdd1.filter(w -> w.matches("p\\d{3}")); // p가 있고 뒤에 3개의숫자가 있음
		System.out.println(rdd2.collect());
		
		rdd2 = rdd1.filter(w -> w.matches("p\\d{4}"));
		System.out.println(rdd2.collect());
	
		rdd2 = rdd1.filter(w -> w.matches("\\d+")); // 숫자 한개 이상
		System.out.println(rdd2.collect());
		
		rdd2 = rdd1.filter(w -> w.matches("\\p{Upper}+")); // p이후 대문자한개이상
		System.out.println(rdd2.collect());
		
		rdd2 = rdd1.filter(w -> w.matches("\\p{Punct}+"));
		System.out.println(rdd2.collect());
		
		rdd2 = rdd1.filter(w -> w.matches("\\p{Alpha}+"));
		System.out.println(rdd2.collect());
		
		rdd2 = rdd1.filter(w -> w.matches("[가-힣]+")); // 한글 한개이상
		System.out.println(rdd2.collect());
	}	
}
```

#### RDDOperate7.java

```java
package sparkexam;

import java.util.Arrays;
import java.util.List;

import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function2;

public class RDDOperate7 {

	public static void main(String[] args) throws Exception {
		JavaSparkContext sc = SparkUtils.getSparkContext("test7");

		doReduce(sc);

		sc.stop();
	}

	public static void doReduce(JavaSparkContext sc) {
		List<Integer> data = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);
		JavaRDD<Integer> rdd = sc.parallelize(data, 3); // 3개의 파티션으로 나눔
		// Java7
		//type parameter가 3개가 있어야한다
		// 첫번째 두번째 아규먼트는 call 메서드 타입, 3번째는 return값의 타입
		// reduce는 직계낸다의 의미, autounboxing에 의해서 int변수로 저장할 수 있음
		int result = rdd.reduce(new Function2<Integer, Integer, Integer>() {
			@Override
			public Integer call(Integer v1, Integer v2) throws Exception {
				return v1 + v2;
			}
		});
		
		System.out.println(result);
		// Java8
		int result2 = rdd.reduce((v1, v2) -> v1 + v2);
		System.out.println(result2);
	}
}
```

#### RDDOperate8.java

```java
package sparkexam;

import java.util.Arrays;
import java.util.List;

import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function2;

import scala.Tuple2;

public class RDDOperate8 {

	public static void main(String[] args) throws Exception {
		JavaSparkContext sc = SparkUtils.getSparkContext("test8");
		doReduceByKey(sc);
		sc.stop();
	}

	public static void doReduceByKey(JavaSparkContext sc) {
		// key는 String, value는 Integer
		List<Tuple2<String, Integer>> data = Arrays.asList(new Tuple2<>("a", 1), new Tuple2<>("b", 1), new Tuple2<>("b", 1));

		// parallelize는 한가지 타입이고
		// parallelizePairs는 Tuple 타입이다.
		// JavaPairRDD key타입과 vallue를 모두 줘야한다.Map과 비슷, key단위로 묶어서 처리할 때 사용한다.
		JavaPairRDD<String, Integer> rdd = sc.parallelizePairs(data);
		
		// RDD가 값만 가지고 있으면 reduce만 사용하면 되지만
		// RDD가 key value를 가지고 있으면 reduceByKey를 사용한다.
		// Java7
		JavaPairRDD<String, Integer> result = rdd.reduceByKey(new Function2<Integer, Integer, Integer>() {
			@Override
			public Integer call(Integer v1, Integer v2) throws Exception {
				return v1 + v2;
			}
		});
		System.out.println(result.collect());
		// Java8 Lambda
		JavaPairRDD<String, Integer> result2 = rdd.reduceByKey((v1,  v2) -> v1 + v2);
		System.out.println(result2.collect());
		
		// 변수명을 tupledata라고 안해도 되고 아무거나 상관없다.
		// _1 : key, _2 : value
		result2.foreach(tupledata -> 
		System.out.println(tupledata._1 + "==="+tupledata._2));
	}

}
```

#### RDDOperate9.java

```java
package sparkexam;

import java.util.Arrays;

import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

import scala.Tuple2;

public class RDDOperate9 {

	public static void main(String[] args) throws Exception {
		JavaSparkContext sc = SparkUtils.getSparkContext("test9");

		doMap(sc);

		sc.stop();
	}

	public static void doMap(JavaSparkContext sc) {
		
		JavaRDD<String> rdd1 = sc.textFile("hdfs://192.168.111.120:9000/edudata/fruits.txt");
		
		//행 단위로 블랭크로 구분
		JavaRDD<String> rdd2 = rdd1.flatMap((String line) -> Arrays.asList(line.split("[\\s]+")).iterator());
		System.out.println(rdd2.collect());
		JavaRDD<String> rdd3 = rdd2.filter(word -> word.length() > 4); // word길이가 4가 빠지게 된다. pear
		System.out.println(rdd3.collect());
		// mapToPair은 리턴값이 Tuple값이어야한다.
		// key : String , value : Integer
		// mapToPair에 의해서 1하씩 전달될것이다.
		JavaPairRDD<String, Integer> rdd4 = rdd3.mapToPair(word -> new Tuple2<String, Integer>(word, 1));
		//[(apple,1), (banana,1), (cherry,1), (peach,1), (banana,1), (cherry,1), (peach,1), (cherry,1), (cherry,1), (apple,1), (cherry,1), (peach,1), (grape,1), (apple,1), (grape,1), (cherry,1), (peach,1), (orange,1), (banana,1), (cherry,1), (orange,1), (apple,1), (banana,1), (banana,1), (peach,1), (banana,1), (orange,1), (banana,1), (orange,1), (peach,1), (orange,1), (orange,1), (apple,1), (banana,1), (peach,1), (apple,1), (tomato,1), (cherry,1), (peach,1), (tomato,1), (tomato,1), (tomato,1), (tomato,1)]
		System.out.println(rdd4.collect());
		JavaPairRDD<String, Integer> rdd5 = rdd4.reduceByKey((a, b) -> a + b);		
		//[(banana,8), (orange,6), (grape,2), (cherry,8), (tomato,5), (peach,8), (apple,6)]
		System.out.println(rdd5.collect());
	}	
}
```

#### RDDOperate10.java

```java
package sparkexam;

import java.util.Arrays;
import java.util.List;

import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaSparkContext;

import scala.Tuple2;

public class RDDOperate10 {
	public static void main(String[] args) throws Exception {
		JavaSparkContext sc = SparkUtils.getSparkContext("test10");

		doSortByKey(sc);

		sc.stop();
	}

	public static void doSortByKey(JavaSparkContext sc) {
		List<Tuple2<String, Integer>> data = Arrays.asList(new Tuple2<>("q",3), new Tuple2<>("z",8), new Tuple2<>("a", 2));
		JavaPairRDD<String, Integer> rdd = sc.parallelizePairs(data);
		//key를 이용해서 정렬 !!
		JavaPairRDD<String, Integer> result = rdd.sortByKey();
		System.out.println(result.collect()); // defualt 오름차순
		
		JavaPairRDD<Integer, String> rdd1 = rdd.mapToPair(x -> x.swap());
		JavaPairRDD<Integer, String> rdd2 = rdd1.sortByKey(false); // 내림차순
		// key와 value를 swap -> key를 가지고 sort하는 방법 밖에 없으니 이렇게 한다.
		JavaPairRDD<String, Integer> rdd3 = rdd2.mapToPair(x -> x.swap());
		
		rdd3.foreach(item -> System.out.println(item));		
	}
}
```

#### RDDOperate11.java

```java
package sparkexam;

import java.util.Arrays;

import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

public class RDDOperate11 {

	public static void main(String[] args) throws Exception {
		JavaSparkContext sc = SparkUtils.getSparkContext("test11");

		doZip(sc);

		sc.stop();
	}

	public static void doZip(JavaSparkContext sc) {
		JavaRDD<String> rdd1 = sc.parallelize(Arrays.asList("a", "b", "c"));
		JavaRDD<Integer> rdd2 = sc.parallelize(Arrays.asList(1, 2, 3));
		// key,value 결합
		JavaPairRDD<String, Integer> result = rdd1.zip(rdd2);
		System.out.println(result.collect());
		
		JavaRDD<String> rdd3 = sc.parallelize(Arrays.asList("a", "b", "c"));
		JavaRDD<String> rdd4 = sc.parallelize(Arrays.asList("a", "e", "f"));
		// union(rdd4) : 결합
		JavaRDD<String> result2 = rdd3.union(rdd4);
		System.out.println(result2.collect());
		
		// distinct() 중복 제거 !! unique하게 만든다.
		JavaRDD<String> result3 = result2.distinct();
		System.out.println(result3.collect());
	}

}
```

#### RDDOperate12.java

```java
package sparkexam;

import java.util.Arrays;
import java.util.List;

import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaSparkContext;

import scala.Tuple2;

public class RDDOperate12 {

	public static void main(String[] args) throws Exception {
		JavaSparkContext sc = SparkUtils.getSparkContext("test12");

		doJoin(sc);

		sc.stop();
	}

	public static void doJoin(JavaSparkContext sc) {
		List<Tuple2<String, Integer>> data1 = Arrays.asList(new Tuple2<>("a", 1), new Tuple2<>("b", 1), new Tuple2<>("c", 1),
				new Tuple2<>("d", 1), new Tuple2<>("e", 1));
		List<Tuple2<String, Integer>> data2 = Arrays.asList(new Tuple2<>("b", 2), new Tuple2<>("c", 2));

		JavaPairRDD<String, Integer> rdd1 = sc.parallelizePairs(data1);
		JavaPairRDD<String, Integer> rdd2 = sc.parallelizePairs(data2);

		JavaPairRDD<String, Tuple2<Integer, Integer>> result = rdd1.<Integer>join(rdd2);
		System.out.println(result.collect());

	}

}
```

#### RDDOperate13.java

```java
package sparkexam;

import java.util.Arrays;

import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;

public class RDDOperate13 {

	public static void main(String[] args) throws Exception {
		JavaSparkContext sc = SparkUtils.getSparkContext("test13");

		doGroupBy(sc);

		sc.stop();
	}

	public static void doGroupBy(JavaSparkContext sc) {
		JavaRDD<Integer> rdd1 = sc.parallelize(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10));

		// Java7
		// 첫번째 타입 파라미터, 두번째 리턴타입
		JavaPairRDD<String, Iterable<Integer>> rdd2 = rdd1.groupBy(new Function<Integer, String>() {
			@Override
			public String call(Integer v1) throws Exception {
				return (v1 % 2 == 0) ? "even" : "odd";
			}
		});

		// Java8 Lambda
		// 그룹단위로 처리가 가능하다.
		JavaPairRDD<String, Iterable<Integer>> rdd3 = rdd1.groupBy((Integer v1) -> (v1 % 2 == 0) ? "even" : "odd");

		System.out.println(rdd2.collect());
		System.out.println(rdd3.collect());
	}
}
```

#### WordCount7.java

- java 7

```java
package sparkexam;

import java.util.Arrays;
import java.util.Iterator;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;

import scala.Tuple2;

public class WordCount7 {

	public static void main(String[] args) {
		SparkConf conf = new SparkConf().setAppName("UNICO").setMaster("local");
		JavaSparkContext sc = new JavaSparkContext(conf);
	
		JavaRDD<String> rdd1 = sc.textFile("hdfs://192.168.111.120:9000/edudata/fruits.txt");

		JavaRDD<String> rdd2 = rdd1.flatMap(new FlatMapFunction<String, String>() {
			@Override
			public Iterator<String> call(String v) throws Exception {
				return Arrays.asList(v.split("[\\s]+")).iterator();
			}
		});

		JavaPairRDD<String, Long> rdd3 = rdd2.mapToPair(new PairFunction<String, String, Long>() {
			@Override
			public Tuple2<String, Long> call(String s) throws Exception {
				return new Tuple2<String, Long>(s, 1L);
			}
		});

		JavaPairRDD<String, Long> rdd4 = rdd3.reduceByKey(new Function2<Long, Long, Long>() {
			@Override
			public Long call(Long v1, Long v2) throws Exception {
				return v1 + v2;
			}
		});
		
		System.out.println(rdd4.collect());
		rdd4.saveAsTextFile("hdfs://192.168.111.120:9000/result/sparkoutput10");
		sc.close();
		sc.stop();

	}

}

```

#### WordCount8.java

- java 8

```java
package sparkexam;

import java.util.Arrays;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

import scala.Tuple2;

public class WordCount8 {

	public static void main(String[] args) throws Exception {
		SparkConf conf = new SparkConf().setAppName("WordCount_version8").setMaster("local");
	
		JavaSparkContext sc = new JavaSparkContext(conf);

		JavaRDD<String> rdd1 = sc.textFile("hdfs://192.168.111.120:9000/edudata/fruits.txt");
	

		JavaRDD<String> rdd2 = rdd1.flatMap(line -> Arrays.asList(line.split("[\\s]+")).iterator());

		JavaPairRDD<String, Long> rdd3 = rdd2.mapToPair(w -> new Tuple2<String, Long>(w, 1L));

		JavaPairRDD<String, Long> rdd4 = rdd3.reduceByKey((x, y) -> x + y);
		
		rdd4.saveAsTextFile("hdfs://192.168.111.120:9000/result/sparkoutput12");
		Thread.sleep(50000);
		sc.close();

	}
}
```



### 정규표현식

`.`

개행 문자를 제외한 모든 단일 문자와 대응됩니다.

예를 들어, `/.n/`는 "nay, an apple is on the tree"에서 'an'과 'on'에 대응되지만, 'nay' 에는 대응되지 않습니다.

`*`

앞의 표현식이 0회 이상 연속으로 반복되는 부분과 대응됩니다. {0,} 와 같은 의미입니다.

예를 들어, `/bo*/` 는 "A ghost booooed" 의 'boooo' 와 대응되고, "A bird warbled" 의 'b'에 대응되지만 "A goat grunted" 내의 어느 부분과도 대응되지 않습니다.
`+`

앞의 표현식이 1회 이상 연속으로 반복되는 부분과 대응됩니다. `{1,}` 와 같은 의미입니다.

예를 들어, `/a+/` 는 "candy"의 'a'에 대응되고 "caaaaaaandy" 의 모든 'a'들에 대응되지만, "cndy" 내의 어느 부분과도 대응되지 않습니다.

----

- cmd창에서 실행~!!!

```
val rdd1 = sc.textFile("hdfs://192.168.111.120:9000/edudata/fruits.txt")
val rdd2 = rdd1.flatMap(line => line.split(" "))
val rdd3 = rdd2.map(word => (word, 1))
val rdd4 = rdd3.reduceByKey(_ + _)
rdd4.collect()
```

cmd 창에서 `spark-shell` 명령어를 작성하고 spark을 실행 시킨후 한줄씩 작성한다.

http://localhost:4040을 크롬에가서 이동하고 확인한다. -> SparkContext가 기동되는 동안에만 허용됨 !! 

cmd창에서는 :q하고 spark-shell을 종료하면 주소가 허용이 되지 않는다.

Spark이 어떤 일을 했는지에대해서 볼수 있고 비주얼하게도 확인이 가능하다.