# 인공지능 개념 이해 

### 목적

- 코드와 시각적으로 딥러닝 이해
- 깊게 들어가는 이론은 없으니 먼저 쭉 따라 해보시다가 이론 공부 하신 후에 다시 한번 더 보시는걸 권장
- 일단 기본 CNN과 간단한 프로젝트도 돌릴 수 있을 정도의 수준

### 강의 구성

- CNN을 이해하기 위한 전체적인 큰 그림
  - CNN을 이해하기 위해 알아야 할 것
- CNN을 짜기 위한 준비
  - Anaconda 설치부터 여는 방법
  - Numpy부터 matplotlib 기초

## 딥러닝 전체 구조 및 학습 과정

Data -> Model ->  logit -> Result

​				↑			  ↑

​			Optm   <-  Loss

Data가 입력 되면 Model이라는게 학습이 된다. 그 후 logit에서 예측을 하고 Data를 줄때 Object를 같이 주게 되는데 정답도 같이 준다. 정답은 label이라는 것이다. label과 예측 결과와 얼마나 틀렸는지 확인하고 에러률을 최소화 하기 위해 적용하고 다시 최소화한것을 Model에 적용한다. 이것이 우리가 원하는 만큼 무한정으로 반복 되다가 학습이 되고 나서 멈춘다. 그때 결과가 나온다. 마치 학생이 문제 푸는 과정과 같다.

#### Data(중요)

- 학습 시키기 위한 데이터. 이 데이터가 모델에 들어감
- 데이터가 생성되고, 데이터에 Transform 변형을 준다거나 모델에 들어가기 전에 데이터 전처리가 들어감
- 이 때 들어갈 때는 Batch로 만들어서 Model에 넣어줌

#### Model

- LeNet, AlexNet, VGG나 ResNet 등 다양하게 설계된 모델
- Convolution Layer, Pooling 등 다양한 Later 층들로 구성
- 이 모델 안에 학습 파라미터가 있고, 이 모델이 학습하는 대상

#### Prediction / Logit

- 마치 문제를 푸는 것이다.
- [0.15, 0.3, 0.2, 0.25, 0.1]
  - 각 Class별로 예측한 값.
  - 여기서 가장 높은 값이 모델이 예상하는 class 또는 정답
- [0.0, 0.0, 0.0, 1.0, 0.0]
  - 위의 숫자가 정답이라고 할 때 얼마나 틀렸는지 얼마나 맞았는지 확인 가능

#### Loss / Cost

- 예측한 값과 정답과 비교해서 얼마나 틀렸는지를 확인.
- cross Entorpy등 다양한 Loss Function들이 있음.
- 이 때 계산을 통해 나오는 값이 Loss(Cost, Cost Value 등)이라고 불림
- 이 Loss는 "얼마나 틀렸는지"를 말하며 이 값을 최대한 줄이는 것이 학습의 과정

#### Optimization

- 앞에서 얻은 Loss 값을 최소화하기 위해 기울기를 받아 최적화된 Variable 값들로 반환
- 이 반환된 값이 적용된 모델은 바로 전에 돌렸을 때으 결과보다 더 나아지게 됨.
- 이 때 바로 최적화된 값만큼 바로 움직이는 것이 아닌 Learning Rate만큼 움직인 값이 적용

#### Result

- 평가 할 때 또는 예측된 결과를 확인 할 때는 예측된 값에서 argmax를 통해 가장 높은 값을 예측한 class라고 둠
- [0.15, 0.3, 0.2, 0.25, 0.1]
  - 위의 예측값에서는 0.2가 제일 높은 값이 므로 클래스가 2가 가장 높다고 봄(파이썬에서 0으로 시작)

## 딥러닝 용어

#### Model

https://www.researchgate.net/figure/Structures-of-AlexNet-top-and-structure-of-our-system-with-part-of-AlexNet-applied-as_fig1_314200586

#### Layer

Input layer, Hidden layers, Output layer

층을 여러 층을 쌓았다고해서 Deep Learning이라고 한다.



deep learning CNN model: VGG16

VGG가 모델중에 하나이다.

#### Convolution

- Input image

  ![image-20200303214537122](C:\Users\seouz\AppData\Roaming\Typora\typora-user-images\image-20200303214537122.png)

- Convolution Kernel 

  ![image-20200303214548857](C:\Users\seouz\AppData\Roaming\Typora\typora-user-images\image-20200303214548857.png)

- Feature map

  ![image-20200303214556494](C:\Users\seouz\AppData\Roaming\Typora\typora-user-images\image-20200303214556494.png)

  이미지와 어떤 것을 합성해서 이미지를 뽑아 내는것이 Convolution이다.

#### Weight / Filter / Kernel / Variable / Bias

- Visualize Learned Weights

  ![image-20200303214846360](C:\Users\seouz\AppData\Roaming\Typora\typora-user-images\image-20200303214846360.png)

Convolution 안에서 Weights를 학습시킨다고 생각하면 된다.

#### Pooling Layer

- Convolved feature
- pooled feature

![image-20200303215014762](C:\Users\seouz\AppData\Roaming\Typora\typora-user-images\image-20200303215014762.png)

Convolution은 특징을 뽑아 낸다고 하면 Pooling은 압축을 한다고 생각하면 된다.

#### Optimization

여기 안에 Adam 등 여러가지 방법이 있는데 그 중 하나를 사용해서 새로 업데이트된 모델을 Loss를 줄일 수 있도록 Optimization이 도와준다.

#### Activation Function

![image-20200303215142924](C:\Users\seouz\AppData\Roaming\Typora\typora-user-images\image-20200303215142924.png)

앞에서 특징을 뽑았으면 음수값이나 불필요한 값들을 제거한다.

#### Softmax

![image-20200303215546529](C:\Users\seouz\AppData\Roaming\Typora\typora-user-images\image-20200303215546529.png)

LOGITS SCORE가 가장 큰 쪽이 정답이다라는 것이라고 말한다.

Softmax는 앞에서 받은 값들을 확률로 나타나게 한다.

#### Cost / Loss / Loss Function

0, 1, 2, 3, 4 중에 3 이 정답이라고 하자!!

그러나 컴퓨터는 0, 1, 2, 3, 4의 정답 확률을 모두 계산한다. 얼마나 틀렸는지를 계산하는 방식이 Loss Function 또는 Cost Function이라고 한다. 여기서 나온 수치가 Loss 또는 Cost이다.

![image-20200303220057103](C:\Users\seouz\AppData\Roaming\Typora\typora-user-images\image-20200303220057103.png)


#### Learning Rate

Hyper parameter중에 하나이다.

Hyper parameter : 컴퓨터가 직접 스스로 설정을 할 수 없기 때문에 인간이 직접 설정을 조절 해줘야한다. 그 중 하나가 Learning Rate이다.

![image-20200303220554037](C:\Users\seouz\AppData\Roaming\Typora\typora-user-images\image-20200303220554037.png)

Learning Rate가 너무 낮거나 높아 버리면 이상이 생기기 때문에 적절한 값으로 설정을 해주어야한다. 너무 낮으면 웅덩이에 빠지듯이 너무 낮은 값을 인지해 버릴 수 있고 너무 높으면 낮은 값을 인지 하지 못하기 때문에 적절한 값을 설정해야한다.

#### Batch Size

데이터를 넣는데 모든 데이터를 한번에 넣을 수 없다.

MNIST 가 있는데 데이터만해도 60000개이다. 한번에 넣을 수 가 없다.