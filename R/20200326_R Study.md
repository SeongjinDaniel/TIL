# 20200326 R Study

#### 예제

```R
# 교재 81페이지
library() # 이미 설치된 package를 visual하게 보여준다.
installed.packages() # 이미 설치된 package를 console창에 보여준다.
search() # 이미 로드된 package만 보여준다.
read_excel() # 아직 설치 안되서 찾을 수 없음
install.packages("readxl")
library(readxl) # require(readxl) # 로드
excel_data_ex <- read_excel("book/data_ex.xls")
getwd()
View(excel_data_ex)
search()

# 웹 크롤링과 스크래핑
install.packages("rvest")  # installed 라고 하면 확인 하는 함수!!
library(rvest) # load 해제는 detach

url <- "http://unico2013.dothome.co.kr/crawling/tagstyle.html"
text <- read_html(url) # 크롤링 해서 끌어옴
text # html_document 객체 리턴, 근데 R은 객체 만드는것 없으니까 리스트로 만든것임
str(text)

nodes <- html_nodes(text, "div") # div 태그로 된것만 꺼내줘
nodes
title <- html_text(nodes)
title

node1 <- html_nodes(text, "div:nth-of-type(1)") # nth-of-type(1) :첫번째 div를 찾음
node1
html_text(node1)
html_attr(node1, "style") # 찾아진 노드에 대하여 속성값을 꺼내온다.

node2 <- html_nodes(text, "div:nth-of-type(2)")
node2
html_text(node2)
html_attr(node2, "style") # style 속성값 없음

node3 <- html_nodes(text, "div:nth-of-type(3)")
node3
html_text(node3)
# 없으니까 안함

# 단일 페이지(rvest 패키지 사용)
install.packages("rvest"); 
library(rvest)
text<- NULL
url<- "http://movie.naver.com/movie/point/af/list.nhn?page=1"
text <- read_html(url,  encoding="CP949")
text
# 영화제목
nodes <- html_nodes(text, ".movie")
title <- html_text(nodes)
title
# 영화평점
nodes <- html_nodes(text, ".title em")
point <- html_text(nodes)
point
# 영화리뷰
# css로 불가능해서 xpath로 사용해야함, // : 조상이 누가 있던간에
# tbody는 html에서 table을 쓰면 자동으로 들어감! 직접 지정 가능
# example (크롬에서 우클릭해서 가져올수 있음)
# #old_content > table > tbody > tr:nth-child(2) > td.title > div > em
# //*[@id="old_content"]/table/tbody/tr[2]/td[2]/div/em

# //*[@id="old_content"]/table/tbody/tr[2]/td[2]/text() -> 크롬에서 우클릭해서 copy 할 수 있다.
nodes <- html_nodes(text, xpath="//*[@id='old_content']/table/tbody/tr/td[2]/text()")
imsi <- html_text(nodes, trim=TRUE)
review <- imsi[nchar(imsi) > 0] # 내용이 없는 애들은 빼버리고 내용이 있는 것만 담는다.
?nchar # 문자열의 길이 구함
review
if(length(review) == 10) {
  page <- cbind(title, point)
  page <- cbind(page, review)
  write.csv(page, "movie_reviews.csv")
} else {
  cat("리뷰글이 생략된 데이터가 있네요ㅜㅜ\n")
}

###############################################
# 여러 페이지
site<- "http://movie.naver.com/movie/point/af/list.nhn?page="
text <- NULL
movie.review <- NULL
for(i in 1: 100) {
  url <- paste(site, i, sep="")
  text <- read_html(url,  encoding="CP949")
  nodes <- html_nodes(text, ".movie")
  title <- html_text(nodes)
  nodes <- html_nodes(text, ".title em")
  point <- html_text(nodes)
  nodes <- html_nodes(text, xpath="//*[@id='old_content']/table/tbody/tr/td[2]/text()")
  imsi <- html_text(nodes, trim=TRUE)
  review <- imsi[nchar(imsi) > 0] 
  if(length(review) == 10) {
    page <- cbind(title, point)
    page <- cbind(page, review)
    movie.review <- rbind(movie.review, page)
  } else {
    cat(paste(i," 페이지에는 리뷰글이 생략된 데이터가 있어서 수집하지 않습니다.ㅜㅜ\n"))
  }
}
write.csv(movie.review, "movie_reviews2.csv")


# 한국일보 페이지(XML 패키지 사용)
install.packages("XML")
library(XML)
imsi <- read_html("http://hankookilbo.com")
t <- htmlParse(imsi)
content<- xpathSApply(t,"//p[@class='title']", xmlValue); 
content
content <- gsub("[[:punct:][:cntrl:]]", "", content)
content
content <- trimws(content)
content

# httr 패키지 사용 - GET 방식 요청
install.packages("httr") # version이 안맞으면 다시 리스타트 할래 라고 물어본다.
library(httr)
http.standard <- GET('http://www.w3.org/Protocols/rfc2616/rfc2616.html')
title2 = html_nodes(read_html(http.standard), 'div.toc h2')
title2 = html_text(title2)
title2

# httr 패키지 사용 - POST 방식 요청
library(httr)
# POST 함수를 이용해 모바일 게임 랭킹 3월 15일 주  모바일 게임 랭킹을 찾는다
#(http://www.gevolution.co.kr/score/gamescore.asp?t=3&m=0&d=week) 
game = POST('http://www.gevolution.co.kr/score/gamescore.asp?t=3&m=0&d=week',
            encode = 'form', body=list(txtPeriodW = '2020-03-15'))
title2 = html_nodes(read_html(game), 'a.tracktitle')
title2 = html_text(title2)
title2[1:10]


# 뉴스, 게시판 등 글 목록에서 글의 URL만 뽑아내기 
res = GET('https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=001')
htxt = read_html(res)
link = html_nodes(htxt, 'div.list_body a'); length(link)
article.href = unique(html_attr(link, 'href'))
article.href
#############################################

# 이미지, 첨부파일 다운 받기 
# pdf
res = GET('http://cran.r-project.org/web/packages/httr/httr.pdf')
writeBin(content(res, 'raw'), 'c:/Temp/httr.pdf')

# jpg
h = read_html('http://unico2013.dothome.co.kr/productlog.html')
imgs = html_nodes(h, 'img')
img.src = html_attr(imgs, 'src')
for(i in 1:length(img.src)){
  res = GET(paste('http://unico2013.dothome.co.kr/',img.src[i], sep=""))
  writeBin(content(res, 'raw'), paste('c:/Temp/', img.src[i], sep=""))
} 
```

#### daumnews.R

```
url <- "https://media.daum.net/ranking/popular/"
text <- read_html(url)

title <- html_nodes(text, "#mArticle div.cont_thumb > strong > a")
txtTitle <- html_text(title, trim=TRUE)
txtTitle

company <- html_nodes(text, ".info_news")
txtCompany <- html_text(company, trim=TRUE)
txtCompany

daumnews <- data.frame("newstitle"=txtTitle, "newspapername"=txtCompany)
write.csv(daumnews, "C:/oliver_c/Rstudy/oliver_csv/daumnews.csv")
```

#### movie1.R

```R
url <- "https://movie.daum.net/moviedb/grade?movieId=127122"
text <- read_html(url)

reviewScore1 <- html_nodes(text, ".emph_grade")
txttemp1 <- html_text(reviewScore1)
txttemp1

review1 <- html_nodes(text, ".desc_review")
txttemp2 <- html_text(review1, trim=TRUE)
txttemp2

daummovie <- data.frame("grade"=txttemp1, "review"=txttemp2)
write.csv(daummovie, "C:/oliver_c/Rstudy/oliver_csv/daummovie")
```

#### movie2.R

```R
site<- "https://movie.daum.net/moviedb/grade?movieId=127122&type=netizen&page="
text <- NULL
movie.review <- NULL
for(i in 1: 20) {
  url <- paste(site, i, sep="")
  text <- read_html(url)
  
  nodes <- html_nodes(text, ".emph_grade")
  grade <- html_text(nodes)
  
  nodes2 <- html_nodes(text, ".desc_review")
  imsi <- html_text(nodes, trim=TRUE)
  review <- imsi[nchar(imsi) > 0]
  if(length(review) == 10) {
    # 벡터에 append시켜서 해도 된다. 그리고 나서 data.fram에 저장
    page <- cbind(grade)
    page <- cbind(page, review)
    movie.review <- rbind(movie.review, page)
  } else {
    cat(paste(i," 페이지에는 리뷰글이 생략된 데이터가 있어서 수집하지 않습니다.ㅜㅜ\n"))
  }
}
write.csv(movie.review, "C:/oliver_c/Rstudy/oliver_csv/daummovie2.csv")
```



